{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 'simple' bivariate linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get started lets import some libraries that we will definately need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets begin with a simple example. We have two variables (x and y), each with some scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([0,1,2,3,4,5,6,7,8,9])\n",
    "y = np.array([1,3,2,5,7,8,8,9,10,12])\n",
    "\n",
    "plt.scatter(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to find the best possible straight line to represent these points. That's the challenge in this chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Finding the best line 'by hand'\n",
    "Any straight line can be written as:\n",
    "$y = b_0 + b_1*x$\n",
    "\n",
    "The $b_0$ is known as the constant or intercept and the $b_1$ as the slope or gradient. Both $b_0$ and $b_1$ are sometimes also referred to as coefficients, and sometimes only $b_1$ is deemed a coefficient.\n",
    "\n",
    "We want to choose $b_0$ and $b_1$ in such a way that it minimizes the total difference with the known points. However, not just the normal difference, but actually the squared difference.\n",
    "\n",
    "We could keep on guessing but some smart people did some math for us and came up with two scary looking formulas:\n",
    "\n",
    "\\begin{equation*}\n",
    "b_1=\\frac{\\bar{xy}-\\bar{x}\\times\\bar{y}}{s_x^2}\n",
    "\\end{equation*}\n",
    "\n",
    "And\n",
    "\n",
    "\\begin{equation*}\n",
    "b_0=\\bar{y}-\\bar{x}\\times b_0\n",
    "\\end{equation*}\n",
    "\n",
    "A symbol with a bar on top, simply means average (mean). The $\\bar{xy}$ is the mean of the x values multiplied with y.\n",
    "\n",
    "So lets calculate these for our example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sx2 = x.var()\n",
    "mxy = np.array(x*y).mean()\n",
    "b1=(mxy-x.mean()*y.mean())/sx2\n",
    "b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b0=y.mean()-b1*x.mean()\n",
    "b0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets calculate our predicted values with these values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myPrediction = b0+b1*x\n",
    "\n",
    "plt.scatter(x,y, color='blue')\n",
    "plt.plot(x,myPrediction, color='red')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. How good is the best?\n",
    "\n",
    "How well does our prediction actually work. We could of course simply determine the mean of the differences (the so-called residuals):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average(y-myPrediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That doesn't seem right. We are not far off with the prediction, but this seems ridiculous low. The reason are the negative values, we simply want the difference in absolute values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE = np.mean(np.absolute(y-myPrediction))\n",
    "print(MAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This value is sometimes known as the Mean Absolute Error (MAE). As we saw in the previous session squaring in stead of absolute value is more common in statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE = np.mean((y-myPrediction)**2)\n",
    "print(MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the Mean Squared Error (MSE) and as with the standard deviation, we can take the square root out of this to get the Root Mean Squared Error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE=MSE**(0.5)\n",
    "print(RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, more common to indicate how well a model is predicting the data a so-called coefficient of determination is calculated. This is usually written as $r^2$. You might recognize that $r$ from the previous session, it was the correlation coefficient. One way of calculating the determination coefficient is indeed by simply squaring the correlation coefficient.\n",
    "\n",
    "The coefficient of determination will always be between 0 and 1. It is a percentage of the variance in the dependent variable (y) that is predictable from the independent variable(s) (x).\n",
    "\n",
    "The formula for the correlation coefficient is usually given by:\n",
    "\n",
    "\\begin{equation*}\n",
    "r=\\frac{s_{xy}}{s_x\\times s_y}\n",
    "\\end{equation*}\n",
    "\n",
    "Here $s_{xy}$ is used to indicate the covariance, which in itself can be determined by:\n",
    "\n",
    "\\begin{equation*}\n",
    "s_{xy}=\\frac{\\sum(x-\\bar{x})\\times(y-\\bar{y})}{n-1}\n",
    "\\end{equation*}\n",
    "\n",
    "I'm using here everywhere $s$ which usually is used for a so-called sample standard deviation. This divides by 'n'. However python more often uses $\\sigma$ which is the population standard deviation. In this case it doesn't really matter since they actually will cancel each other out (the n-1 in the covariance will be cancelling the n-1 in the two standard deviations), and especially with big data using n or n - 1 will not lead to a big difference.\n",
    "\n",
    "Okay, lets calculate that correlation coefficient and determination coefficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covxy = np.sum((x-x.mean())*(y-y.mean()))/x.size\n",
    "sX = x.std()\n",
    "sY = y.std()\n",
    "\n",
    "cor = covxy/(sX*sY)\n",
    "print(cor)\n",
    "\n",
    "det = cor**2\n",
    "print(det)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calculation above for the determination coefficient shows the link between the correl and the determination coefficient. However there are other formulas that lead to the same result.\n",
    "\n",
    "The determination coefficient is a percentage. So we can also look at the total variation in the original values:\n",
    "\n",
    "\\begin{equation*}\n",
    "SS_{tot}=\\sum(y-\\bar{y})^2\n",
    "\\end{equation*}\n",
    "\n",
    "Look at how much variation there is left:\n",
    "\n",
    "\\begin{equation*}\n",
    "SS_{res}=\\sum(\\hat{y}-y)^2\n",
    "\\end{equation*}\n",
    "\n",
    "In this equation $\\hat{y}$ are the predicted values. We then divide the two we get the percentage of unexplained variance:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\frac{SS_{res}}{SS_{tot}}\n",
    "\\end{equation*}\n",
    "\n",
    "Since the determinaton coefficient is the percentage of explained variance, we can simply now find the determination coefficient using:\n",
    "\n",
    "\\begin{equation*}\n",
    "r^2=1-\\frac{SS_{res}}{SS_{tot}}\n",
    "\\end{equation*}\n",
    "\n",
    "Using Python we can check this. A small trick makes the calculations a little easier. We already have the standard deviation of y, and then we can use that $SS_y=s_y^2\\times n$. So we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1-np.sum((y-myPrediction)**2)/(sY**2*y.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't worry, you don't have to remember all those formulas. Numpy has you covered. It has a function to determine the correlation coefficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It returns a square matrix of 2x2. It shows the correlation coefficients between all possible pairs. So the 1 in the upper left corner is the correlation between x and x. The 0.97... is the correlation between x and y, and then in the next row we have the correlation between y and x, and finally between y and y. The diagonal will always be 1s.\n",
    "\n",
    "Just to extract the correlation coefficient and get the determination coefficient is fairly easy now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(x,y)[0,1]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as we had before, an extremely small difference which we'll consider a rounding error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Using sklearn\n",
    "\n",
    "Of course there have been others who have done this work for us. \n",
    "We could for example use sklearn (you'd have to install this first) and then you can import:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does require to reshape our x variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xRes = x.reshape((-1,1))\n",
    "yRes = y.reshape((-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform the regression analysis and saving the predicted results we can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression().fit(xRes,yRes)\n",
    "yPred = model.predict(xRes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder all the values we have calculated so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The slope (b1): ',b1)\n",
    "print('The intercept (b0): ',b0)\n",
    "print('Mean Absolute Error:', MAE)\n",
    "print('Mean Squared Error: ', MSE)\n",
    "print('Root Mean Squared Error: ', RMSE)\n",
    "print('Coefficient of determination: ',det)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets see and compare the result with using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1V2=model.coef_[0]\n",
    "print('The slope (b1): ',b1V2[0])\n",
    "b0V2=model.intercept_\n",
    "print('The intercept (b0): ',b0V2[0])\n",
    "MAE2=metrics.mean_absolute_error(yRes,yPred)\n",
    "print('Mean Absolute Error:', MAE2)\n",
    "MSE2=metrics.mean_squared_error(yRes,yPred)\n",
    "print('Mean Squared Error: ', MSE2)\n",
    "RMSE2=metrics.mean_squared_error(yRes,yPred, squared=True)\n",
    "print('Root Mean Squared Error: ', RMSE2)\n",
    "det2=metrics.r2_score(yRes,yPred)\n",
    "print('Coefficient of determination: ',det2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very small difference with our 'manual' formula for b1 and b0. We'll leave that as a rounding error :-)\n",
    "\n",
    "You might notice I've used an index for the b1 coefficient, since we can actually also have multiple variables to use for our prediction. More on this later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2. Exercise\n",
    "On Moodle you will find a file Soccer2019C.csv. We want to predict the Overall score of players solely based on their age. To load the data we can use pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once pandas is imported we can read a file as a pandas dataframe. If your file is in a separate folder 'data' we could use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soccerDF=pd.read_csv('data/Soccer2019C.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is loaded you can get a quick overview using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soccerDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your exercise is to find the linear regression equation to predict the Overall score, based on the age.\n",
    "\n",
    "There are different ways you can do this:\n",
    "1. Manually\n",
    "2. Using the sklearn library\n",
    "3. Using the statsmodels.api (not discussed yet)\n",
    "\n",
    "You might have to convert the panda dataframe into a numpy array first. Try to find the regression equation with one (or even more to see if they all say the same).\n",
    "\n",
    "Other things you could do if you think you're done....\n",
    "* Add a visualisation\n",
    "* Create a Python function to perform the manual calculations\n",
    "* Find out which variable has the strongest determination coefficient to predict the Overall score\n",
    "* Find out which two variables (one as predictor (x), one as predicted (y)) will have the strongest determination coefficient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age = soccerDF[\"Age\"].to_numpy()\n",
    "score = soccerDF[\"Overall\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(age, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = age.reshape((-1,1))\n",
    "y = score.reshape((-1,1))\n",
    "\n",
    "model = LinearRegression().fit(x, y)\n",
    "y_prediction = model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(age, score, color=\"blue\")\n",
    "plt.plot(x, y_prediction, color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}